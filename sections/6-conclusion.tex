\chapter{Conclusion}
% What I did, what went right, what went wrong, what further work could be done.
\label{chap:con}
This final part3 discusses the results of Section \ref{ap:fullres} in Section \ref{sec:conc:dis}. Future work is outlined in Section \ref{sec:conc:fw}. A summary of this dissertation is provided in Section \ref{sec:conc:ref}. Finally, a personal reflection is provided in Section \ref{sec:conc:ps}.

\section{Discussion Of Results}
\label{sec:conc:dis}
\subsection{Baseline}
The baseline for this dissertation is training the V2V-Posenet model on the MSRA training dataset. As anticipated, it performs well when tested on the {\slshape global error} metric, matching state of the art performance at 10.33mm, the slightly higher error value on the {\slshape local error} indicates that the V2V-Posenet model is also correctly predicting the global position of the hand. The results of the Random MANO test dataset are poor, and this is understandable, given that the datasets inevitably cover a different range of hand poses, however the results of the IK MANO dataset are more concerning. The IK MANO dataset is generated from the labels of the MSRA dataset, so in an ideal world, it should have similar performance, so this alone provides early evidence that the attempt to recreate the MSRA dataset synthetically failed.

\subsection{Experiments}
The following two sections describe the scenarios where the V2V-Posenet model is trained on the two respective synthetic test datasets. See Section \ref{es:exp} for details.

\subsubsection{Random MANO Dataset}
\label{conc:rmd:disc}
This section discusses the results when the V2V-Posenet model is trained on the Random MANO dataset. 

It is first worth noting that these two datasets cover different kinds of poses. The MSRA dataset is a series of frames from a video where 10 subjects perform 17 different hand poses. Given this scenario, many of the frames in the MSRA dataset are similar, with only slight variations between frames for camera angle and pose. In training V2V-Posenet however, depthmap images are passed in at random. In contrast, the Random MANO dataset is entirely random, the hand shape changes in each frame, so in theory, if there are 10,000 images, this could have come from 10,000 different subjects. Each pose is entirely drawn from random, so no two images will be similar in the same way that they are in the MSRA dataset which comes from a video sequence. As well as this, the Random MANO dataset is likely to have higher quality images, while the MSRA dataset is likely to contain noise due to the nature of real world sensors. These differences are important when weighing the differences between these two results. Both the {\slshape global error} and {\slshape local error} metrics perform well when tested with this synthetic dataset, and better than the MSRA dataset. However, the {\slshape local error} performs better which indicates that the {\slshape global error} might improve if the global position of the hands is predicted better, which may indicate a problem with the MANO vertices-to-joints regressor. In my opinion, this dataset performed better than the MSRA dataset because the quality of the labels was better, they were more consistent with respect to each image. The MANO model can output accurate labels, while the MSRA dataset determined the labels using an optimisation algorithm \cite{sun2015cascaded} which will not always find the correct annotation. Indeed, in the ICVL Dataset \cite{tang2016latent}, they admit that when the labelling algorithm fails, they just discard the image leading to an easier dataset to train on (less challenging images). It is curious that the cross comparing the MSRA dataset and Random MANO datasets does not perform well (i.e. training on one dataset and testing the trained model on the other dataset). While both datasets cover different ranges of poses, I do not think that that is the reason. Instead, a possible explanation is that there is a discrepancy in the domain of the groundtruth $\bm{\Phi}$ for a given depthmap image $Z$. The V2V-Posenet model learns to map a depthmap image to a particular domain of predictions to satisfy the {\slshape global error} metric, but given another dataset, it may map to a different domain of predictions. For example, for a depthmap image where the average foreground pixel value is 300, and for that type of image where the average foreground image is 300, it has a groundtruth where the average $z$ value $\bm{\Phi}_{n,2}$ is 100, but in a {\slshape different} dataset, that same type of depthmap image might map to an average $\bm{\Phi}_{n,2}$ of 200. As long as the annotation mechanism is consistent, this domain issue is not a problem for datasets, but it is for comparing datasets. I am not aware of any recent dataset that makes use of data from previous datasets, and I think this is quite telling. The key theme of this dissertation has been the lack of data for hand tracking datasets, and yet, when an attempt is made to create a new dataset, while past techniques are considered, no author ever {\slshape uses} the previous data, and I believe this can be partly explained by this domain issue. Therefore, that is why I think the MSRA and Random MANO datasets do not compare well with each other.

\subsubsection{IK MANO Dataset}
This section discusses the results when the V2V-Posenet model is trained on the IK MANO dataset, the dataset that is generated within MAYA to recreate the MSRA dataset synthetically. The results overall are poor, with test results poor even on the test version of the synthetic dataset. The {\slshape local error} does significantly better than the {\slshape global error} when tested with the MANO dataset in both MSRA weights and random weights scenarios, and this to me indicates a problem with the camera step, that the global position of the hand in the groundtruth with respect to the images is inconsistent. Looking at the sample images in Figure \ref{fig:sd:d}, the hand articulation is generally correct, but the actual orientation of the hand varies widely. The camera orientation was chosen by looking at the orientation of the hand, since there was not information about the camera in the original annotation of the dataset, which clearly is not a good approach.

The method for for deforming the hand joints has shown promise, but the method for finding the camera location was not robust.

\subsubsection{Results Conclusion}
The purpose of this dissertation was to investigate the merits of using synthetic data to train hand tracking systems. Given the results discussed in Section \ref{conc:rmd:disc}, I believe that the use of synthetic data is a valid step in improving the performance of hand tracking systems, but more work is required. These experiments have shown that it is easier to generate synthetic data directly from the MANO model. In contrast, generating synthetic data from the grountruth of a real dataset is harder, since it involves the use of a dubious inverse kinematics algorithm in the absence of proper camera annotation of that real dataset.

\section{Future Work}
\label{sec:conc:fw}
Generating synthetic data at random proved to be easier than making a pipeline for reverse engineering the grountruth. The error of the Random dataset was very low, far lower than the equivalent on the MSRA dataset. It would therefore be interesting to make a very big dataset, in the order of $10^7$ or $10^8$ to see how it performs on the V2V-Posenet model in comparison to a real dataset. As well as this, the problems reverse engineering the grountruth could be fixed, and hand shape could be factored in by calculating the bone lengths in the grountruth and deforming the model in Maya as appropriate. This grountruth-reversal method could also provide a way to augment real datasets like Bighand2.2m. An example of this could be to supplement this dataset with synthetic counterparts, using the grountruth of this dataset as a starting point, the angles of the individual joint deformations could be adjusted, the camera could also be adjusted. In addition to this, a synthetic dataset with hand-object interactions could be investigated for depth images like is done for RGB images in \cite{mueller2017real}.

\section{Summary}
\label{sec:conc:ref}
In this dissertation, a comprehensive evaluation of synthetic data in 2020 was performed using the MANO model as the state-of-the-art in generating synthetic data, with a view towards addressing the lack-of-data problem in hand tracking. Two regimens were demonstrated for generating such data: at random (The Random MANO Dataset), and based on another dataset (The IK MANO Dataset). The random generation method worked by drawing random parameters from a pre-selected uniform distribution for camera, hand articulation, and hand shape. The method where the synthetic data is based on another dataset worked by using inverse kinematics to deform the MANO model within MAYA, with a view towards augmentation of a real dataset.

\section{Postscript}
\label{sec:conc:ps}
This dissertation marks the concluding work that I submit as part of a five year Engineering programme at Trinity College Dublin. This degree has taken me on a journey from a churlish youth to what I hope is someone prepared to take on life ahead. In the midst of submitting this dissertation, the world at large is in lockdown, and it is probably one of the worst times to be a graduate, but life must go on. I will be in a lifetime of emotional debt to all of the great people I encountered through my time here, and I mean this in a most sincere way. A special word of mention is in order towards {\slshape Dublin University Boat Club}, I will forever hold fond memories of both on-water and off-water endeavors. I often suspected, that I spent more time in boats than in lecture theaters.